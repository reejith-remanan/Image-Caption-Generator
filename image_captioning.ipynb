{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"K2s1A9eLRPEj"},"source":["##### Copyright 2018 The TensorFlow Authors.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRLVEKiTEn04"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EFwSaNB8jF7s"},"source":["<style>\n","td {\n","  text-align: center;\n","}\n","\n","th {\n","  text-align: center;\n","}\n","</style>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Cffg2i257iMS"},"source":["# Image captioning with visual attention\n","\n","<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/image_captioning\">\n","    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n","    View on TensorFlow.org</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n","    Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/image_captioning.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n","    View source on GitHub</a>\n","  </td>\n","  <td>\n","    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/image_captioning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n","  </td>\n","</table>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QASbY_HGo4Lq"},"source":["Given an image like the example below, your goal is to generate a\n","caption such as \"a surfer riding on a wave\".\n","\n","<table style=\"text-align: center;\">\n","<tr>\n","  <td>\n","   <img src=\"https://tensorflow.org/images/surf.jpg\"/>\n","  </td>\n","</tr>\n","<tr>\n","  <th>A man surfing, from <a href=https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg>wikimedia</a></th>\n","</tr>\n","</table>\n","\n","The model architecture used here is inspired by [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044), but has been updated to use a 2-layer Transformer-decoder. To get the most out of this tutorial you should have some experience with [text generation](https://www.tensorflow.org/text/tutorials/text_generation),  [seq2seq models & attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention), or [transformers](https://www.tensorflow.org/text/tutorials/transformer)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6HbD8n0w7d3F"},"source":["The model architecture built in this tutorial is shown below. Features are extracted from the image, and passed to the cross-attention layers of the Transformer-decoder.\n","\n","<table>\n","<tr>\n","  <th>The model architecture</th>\n","</tr>\n","<tr>\n","  <td>\n","   <img width=400 src=\"https://tensorflow.org/images/tutorials/transformer/ImageCaptioning.png\"/>\n","  </td>\n","</tr>\n","</table>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1IxifZKT6vXQ"},"source":["The transformer decoder is mainly built from attention layers. It uses self-attention to process the sequence being generated, and it uses cross-attention to attend to the image.\n","\n","By inspecting the attention weights of the cross attention layers you will see what parts of the image the model is looking at as it generates words.\n","\n","![Prediction](https://tensorflow.org/images/imcap_prediction.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"87us2sLVdwME"},"source":["This notebook is an end-to-end example. When you run the notebook, it downloads a dataset, extracts and caches the image features, and trains a decoder model. It then uses the model to generate captions on new images."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5bwwk4uxRz6A"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69728,"status":"ok","timestamp":1683097268648,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"gc06pTaBbl72","outputId":"dac61660-f90f-4534-a7ff-ed68ae9942f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following packages will be REMOVED:\n","  libcudnn8-dev\n","The following held packages will be changed:\n","  libcudnn8\n","The following packages will be DOWNGRADED:\n","  libcudnn8\n","0 upgraded, 0 newly installed, 1 downgraded, 1 to remove and 22 not upgraded.\n","Need to get 430 MB of archives.\n","After this operation, 1,153 MB disk space will be freed.\n","Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n","Fetched 430 MB in 10s (44.4 MB/s)\n","(Reading database ... 122518 files and directories currently installed.)\n","Removing libcudnn8-dev (8.7.0.84-1+cuda11.8) ...\n","update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n","\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libcudnn8 from 8.7.0.84-1+cuda11.8 to 8.1.0.77-1+cuda11.2\n","(Reading database ... 122485 files and directories currently installed.)\n","Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n","Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.7.0.84-1+cuda11.8) ...\n","Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"]}],"source":["!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18591,"status":"ok","timestamp":1683097287234,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"2R1hQGtZEi8Y","outputId":"9f7dbaeb-fd88-40b6-c246-eb2849937427"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: tensorflow 2.12.0\n","Uninstalling tensorflow-2.12.0:\n","  Successfully uninstalled tensorflow-2.12.0\n","\u001b[33mWARNING: Skipping estimator as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mFound existing installation: keras 2.12.0\n","Uninstalling keras-2.12.0:\n","  Successfully uninstalled keras-2.12.0\n","Found existing installation: tensorflow-metadata 1.13.1\n","Uninstalling tensorflow-metadata-1.13.1:\n","  Successfully uninstalled tensorflow-metadata-1.13.1\n","Found existing installation: protobuf 3.20.3\n","Uninstalling protobuf-3.20.3:\n","  Successfully uninstalled protobuf-3.20.3\n"]}],"source":["!pip uninstall -y tensorflow estimator keras tensorflow-metadata protobuf"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41072,"status":"ok","timestamp":1683097328300,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"E3r3fFe-1aaR","outputId":"e3e33525-8240-4534-b1bd-28571c4afd87"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-text==2.11.0\n","  Downloading tensorflow_text-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow<2.12,>=2.11.0\n","  Downloading tensorflow-2.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text==2.11.0) (0.13.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.2.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.22.4)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (23.3.3)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.4.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.54.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (23.1)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.4.0)\n","Collecting protobuf<3.20,>=3.9.2\n","  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.3.0)\n","Collecting tensorflow-estimator<2.12,>=2.11.0\n","  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.6.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.3.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.32.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (4.5.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (67.7.2)\n","Collecting keras<2.12,>=2.11.0\n","  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.16.0)\n","Collecting tensorboard<2.12,>=2.11\n","  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (16.0.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.14.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.40.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.4.3)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.17.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.27.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.8.1)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.3.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.0.12)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow<2.12,>=2.11.0->tensorflow-text==2.11.0) (3.2.2)\n","Installing collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: tensorboard-data-server\n","    Found existing installation: tensorboard-data-server 0.7.0\n","    Uninstalling tensorboard-data-server-0.7.0:\n","      Successfully uninstalled tensorboard-data-server-0.7.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 1.0.0\n","    Uninstalling google-auth-oauthlib-1.0.0:\n","      Successfully uninstalled google-auth-oauthlib-1.0.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.2\n","    Uninstalling tensorboard-2.12.2:\n","      Successfully uninstalled tensorboard-2.12.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-datasets 4.8.3 requires tensorflow-metadata, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.1 tensorflow-estimator-2.11.0 tensorflow-text-2.11.0\n"]}],"source":["!pip install tensorflow-text==2.11.0"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5125,"status":"ok","timestamp":1683097333421,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"zIgI1t2hsxaL","outputId":"61601be7-b186-40d4-8837-000053da6f6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-metadata==1.12.0\n","  Downloading tensorflow_metadata-1.12.0-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata==1.12.0) (1.59.0)\n","Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata==1.12.0) (1.4.0)\n","Requirement already satisfied: protobuf<4,>=3.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata==1.12.0) (3.19.6)\n","Installing collected packages: tensorflow-metadata\n","Successfully installed tensorflow-metadata-1.12.0\n"]}],"source":["!pip install tensorflow-metadata==1.12.0"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12577,"status":"ok","timestamp":1683097367754,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"q4GDlHqZCf_Q","outputId":"1388f9d4-1d40-404a-846c-08550645b0cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow_datasets==4.8.2\n","  Downloading tensorflow_datasets-4.8.2-py3-none-any.whl (5.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (5.9.5)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (1.14.1)\n","Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (1.2.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (2.3.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (1.4.0)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (2.3)\n","Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (3.19.6)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (2.27.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (4.65.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (8.1.3)\n","Collecting dill\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (1.22.4)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (0.1.8)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets==4.8.2) (0.10.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets==4.8.2) (3.15.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets==4.8.2) (4.5.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets==4.8.2) (5.12.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets==4.8.2) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets==4.8.2) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets==4.8.2) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets==4.8.2) (2.0.12)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow_datasets==4.8.2) (1.16.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow_datasets==4.8.2) (1.59.0)\n","Installing collected packages: dill, tensorflow_datasets\n","  Attempting uninstall: tensorflow_datasets\n","    Found existing installation: tensorflow-datasets 4.8.3\n","    Uninstalling tensorflow-datasets-4.8.3:\n","      Successfully uninstalled tensorflow-datasets-4.8.3\n","Successfully installed dill-0.3.6 tensorflow_datasets-4.8.2\n"]}],"source":["!pip install tensorflow_datasets==4.8.2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47730,"status":"ok","timestamp":1682918364727,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"5Xbt8BkPv8Ou","outputId":"3d5ffe0b-1121-450c-a9bd-61dc244e8870"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow_text\n","  Downloading tensorflow_text-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow\n","  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.10/dist-packages (4.8.3)\n","Collecting tensorflow_datasets\n","  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (0.13.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Collecting keras<2.13,>=2.12.0\n","  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.1.8)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.27.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (5.9.5)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.3)\n","Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.2.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.10.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (8.1.3)\n","Collecting array-record\n","  Downloading array_record-0.2.0-py310-none-any.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.13.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (4.65.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (5.12.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.15.0)\n","Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n","Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.15)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.59.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n","Installing collected packages: keras, array-record, tensorflow_datasets, tensorflow, tensorflow_text\n","  Attempting uninstall: tensorflow_datasets\n","    Found existing installation: tensorflow-datasets 4.8.3\n","    Uninstalling tensorflow-datasets-4.8.3:\n","      Successfully uninstalled tensorflow-datasets-4.8.3\n","Successfully installed array-record-0.2.0 keras-2.12.0 tensorflow-2.12.0 tensorflow_datasets-4.9.2 tensorflow_text-2.12.1\n"]}],"source":["#!pip install -U tensorflow_text tensorflow tensorflow_datasets"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7075,"status":"ok","timestamp":1683097384484,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"7TGZmOuqMia9","outputId":"39b3b833-91c8-4772-f1b9-02e5342bede4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.6.1\n"]}],"source":["!pip install einops"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nQ6q39Vd-y-7"},"source":["This tutorial uses lots of imports, mostly for loading the dataset(s)."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5993,"status":"ok","timestamp":1683097398193,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"U8l4RJ0XRPEm"},"outputs":[],"source":["#@title\n","import concurrent.futures\n","import collections\n","import dataclasses\n","import hashlib\n","import itertools\n","import json\n","import math\n","import os\n","import pathlib\n","import random\n","import re\n","import string\n","import time\n","import urllib.request\n","\n","import einops\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","import requests\n","import tqdm\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","import tensorflow_datasets as tfds"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Kl9qGnjWrv80"},"source":["## [Optional] Data handling\n","\n","This section downloads a captions dataset and prepares it for training. It tokenizes the input text, and caches the results of running all the images through a pretrained feature-extractor model. It's not critical to understand everything in this section.\n","\n"," <section class=\"expandable tfo-display-only-on-site\">\n"," <button type=\"button\" class=\"button-red button expand-control\">Toggle section</button>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"q5e_SigQFiWf"},"source":["### Choose a dataset\n","\n","This tutorial is set up to give a choice of datasets. Either [Flickr8k](https://www.ijcai.org/Proceedings/15/Papers/593.pdf) or a small slice of the [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/) dataset. These two are downloaded and converted from scratch, but it wouldn't be hard to convert the tutorial to use the caption datasets available in [TensorFlow Datasets](https://www.tensorflow.org/datasets): [Coco Captions](https://www.tensorflow.org/datasets/catalog/coco_captions) and the full [Conceptual Captions](https://www.tensorflow.org/datasets/community_catalog/huggingface/conceptual_captions).\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wqGXX9Dc5c0v"},"source":["#### Flickr8k"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1683097398194,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"kaNy_l7tGuAZ"},"outputs":[],"source":["def flickr8k(path='flickr8k'):\n","  path = pathlib.Path(path)\n","\n","  if len(list(path.rglob('*'))) < 16197:\n","    tf.keras.utils.get_file(\n","        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n","        cache_dir='.',\n","        cache_subdir=path,\n","        extract=True)\n","    tf.keras.utils.get_file(\n","        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n","        cache_dir='.',\n","        cache_subdir=path,\n","        extract=True)\n","    \n","  captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n","  captions = (line.split('\\t') for line in captions)\n","  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n","\n","  cap_dict = collections.defaultdict(list)\n","  for fname, cap in captions:\n","    cap_dict[fname].append(cap)\n","\n","  train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n","  train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n","\n","  test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n","  test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n","\n","  train_ds = tf.data.experimental.from_list(train_captions)\n","  test_ds = tf.data.experimental.from_list(test_captions)\n","\n","  return train_ds, test_ds"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zQICBAF4FmSL"},"source":["#### Conceptual Captions"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683097400501,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"vQwnxXZXRl12"},"outputs":[],"source":["def conceptual_captions(*, data_dir=\"conceptual_captions\", num_train, num_val):\n","  def iter_index(index_path):\n","    with open(index_path) as f:\n","      for line in f:\n","        caption, url = line.strip().split('\\t')\n","        yield caption, url\n","\n","  def download_image_urls(data_dir, urls):\n","    ex = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n","    def save_image(url):\n","      hash = hashlib.sha1(url.encode())\n","      # Name the files after the hash of the URL.\n","      file_path = data_dir/f'{hash.hexdigest()}.jpeg'\n","      if file_path.exists():\n","        # Only download each file once.\n","        return file_path\n","\n","      try:\n","        result = requests.get(url, timeout=5)\n","      except Exception:\n","        file_path = None\n","      else:\n","        file_path.write_bytes(result.content)\n","      return file_path\n","    \n","    result = []\n","    out_paths = ex.map(save_image, urls)\n","    for file_path in tqdm.tqdm(out_paths, total=len(urls)):\n","      result.append(file_path)\n","\n","    return result\n","\n","  def ds_from_index_file(index_path, data_dir, count):\n","    data_dir.mkdir(exist_ok=True)\n","    index = list(itertools.islice(iter_index(index_path), count))\n","    captions = [caption for caption, url in index]\n","    urls = [url for caption, url in index]\n","\n","    paths = download_image_urls(data_dir, urls)\n","\n","    new_captions = []\n","    new_paths = []\n","    for cap, path in zip(captions, paths):\n","      if path is None:\n","        # Download failed, so skip this pair.\n","        continue\n","      new_captions.append(cap)\n","      new_paths.append(path)\n","    \n","    new_paths = [str(p) for p in new_paths]\n","\n","    ds = tf.data.Dataset.from_tensor_slices((new_paths, new_captions))\n","    ds = ds.map(lambda path,cap: (path, cap[tf.newaxis])) # 1 caption per image\n","    return ds\n","\n","  data_dir = pathlib.Path(data_dir)\n","  train_index_path = tf.keras.utils.get_file(\n","    origin='https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv',\n","    cache_subdir=data_dir,\n","    cache_dir='.')\n","  \n","  val_index_path = tf.keras.utils.get_file(\n","    origin='https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv',\n","    cache_subdir=data_dir,\n","    cache_dir='.')\n","  \n","  train_raw = ds_from_index_file(train_index_path, data_dir=data_dir/'train', count=num_train)\n","  test_raw = ds_from_index_file(val_index_path, data_dir=data_dir/'val', count=num_val)\n","\n","  return train_raw, test_raw"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rBAagBw5p-TM"},"source":["#### Download the dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WFtTZaobquNr"},"source":["The Flickr8k is a good choice because it contains 5-captions per image, more data for a smaller download."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28651,"status":"ok","timestamp":1683097433432,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"EJySPbzJ4Wxw","outputId":"de670db6-0984-4cde-f852-7b3bdeb9c3fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n","1115419746/1115419746 [==============================] - 16s 0us/step\n","Downloading data from https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n","2340801/2340801 [==============================] - 0s 0us/step\n"]}],"source":["choose = 'flickr8k'\n","\n","if choose == 'flickr8k':\n","  train_raw, test_raw = flickr8k()\n","else:\n","  train_raw, test_raw = conceptual_captions(num_train=10000, num_val=5000)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-UAc275FHxm8"},"source":["The loaders for both datasets above return `tf.data.Dataset`s containing `(image_path, captions)` pairs. The Flickr8k dataset contains 5 captions per image, while Conceptual Captions has 1:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":517,"status":"ok","timestamp":1683097441461,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"sAQSps5F8RQI","outputId":"fc932675-c888-4a55-8e53-db55be106369"},"outputs":[{"data":{"text/plain":["(TensorSpec(shape=(), dtype=tf.string, name=None),\n"," TensorSpec(shape=(5,), dtype=tf.string, name=None))"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train_raw.element_spec"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2143,"status":"ok","timestamp":1683097446242,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"xIa0ZaP4tBez","outputId":"e57b7402-d1f5-4087-ba5a-3470452a797a"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(b'flickr8k/Flicker8k_Dataset/2513260012_03d33305cf.jpg', shape=(), dtype=string)\n","tf.Tensor(\n","[b'A black dog is running after a white dog in the snow .'\n"," b'Black dog chasing brown dog through snow'\n"," b'Two dogs chase each other across the snowy ground .'\n"," b'Two dogs play together in the snow .'\n"," b'Two dogs running through a low lying body of water .'], shape=(5,), dtype=string)\n"]}],"source":["for ex_path, ex_captions in train_raw.take(1):\n","  print(ex_path)\n","  print(ex_captions)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8cSW4u-ORPFQ"},"source":["### Image feature extractor\n","\n","You will use an image model (pretrained on imagenet) to extract the features from each image. The model was trained as an image classifier, but setting `include_top=False` returns the model without the final classification layer, so you can use the last layer of feature-maps:  \n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2077,"status":"ok","timestamp":1683097451352,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"IlUckK8Zfikv","outputId":"fd6275ac-a4d9-4dda-b092-95d03da33443"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n","4334752/4334752 [==============================] - 0s 0us/step\n"]}],"source":["IMAGE_SHAPE=(224, 224, 3)\n","mobilenet = tf.keras.applications.MobileNetV3Small(\n","    input_shape=IMAGE_SHAPE,\n","    include_top=False,\n","    include_preprocessing=True)\n","mobilenet.trainable=False"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Dojkiou9gL3R"},"source":["Here's a function to load an image and resize it for the model:"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":385,"status":"ok","timestamp":1683097455473,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"zXR0217aRPFR"},"outputs":[],"source":["def load_image(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.io.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n","    return img"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-JyQ7zS6gzZh"},"source":["The model returns a feature map for each image in the input batch:"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9452,"status":"ok","timestamp":1683097466922,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"sY86n2i6wJNm","outputId":"9c3abad8-2918-4914-e0b7-5602eeb894a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 224, 224, 3)\n","(1, 7, 7, 576)\n"]}],"source":["test_img_batch = load_image(ex_path)[tf.newaxis, :]\n","\n","print(test_img_batch.shape)\n","print(mobilenet(test_img_batch).shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nyqH3zFwRPFi"},"source":["### Setup the text tokenizer/vectorizer\n","\n","You will transform the text captions into integer sequences using the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer, with the following steps:\n","\n","* Use [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt) to iterate over all captions, split the captions into words, and compute a vocabulary of the top words.\n","* Tokenize all captions by mapping each word to its index in the vocabulary. All output sequences will be padded to length 50.\n","* Create word-to-index and index-to-word mappings to display results."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":430,"status":"ok","timestamp":1683097470008,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"NroZIzB90hD3"},"outputs":[],"source":["def standardize(s):\n","  s = tf.strings.lower(s)\n","  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n","  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n","  return s"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683097471818,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"n9SQOXFsyS36"},"outputs":[],"source":["# Use the top 5000 words for a vocabulary.\n","vocabulary_size = 5000\n","tokenizer = tf.keras.layers.TextVectorization(\n","    max_tokens=vocabulary_size,\n","    standardize=standardize,\n","    ragged=True)\n","# Learn the vocabulary from the caption data."]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2141,"status":"ok","timestamp":1683097477099,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"oJGE34aiRPFo","outputId":"014a5a2f-4614-43f9-9262-2fd948ede5f4"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n","Instructions for updating:\n","Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"]}],"source":["tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683097479630,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"oRahTDtWhJIf","outputId":"fb2412f7-1d97-4f65-b87b-d38e4b33bce2"},"outputs":[{"data":{"text/plain":["['', '[UNK]', 'a', '[START]', '[END]', 'in', 'the', 'on', 'is', 'and']"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.get_vocabulary()[:10]"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683097481980,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"-2mGxD33JCxN","outputId":"075b1d34-d340-4f4f-eb0a-8ffb2237116e"},"outputs":[{"data":{"text/plain":["<tf.RaggedTensor [[3, 2, 655, 5, 2, 97, 4], [3, 2, 1937, 10, 4]]>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["t = tokenizer([['a cat in a hat'], ['a robot dog']])\n","t"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1683097484041,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"8Q44tNQVRPFt"},"outputs":[],"source":["# Create mappings for words to indices and indices to words.\n","word_to_index = tf.keras.layers.StringLookup(\n","    mask_token=\"\",\n","    vocabulary=tokenizer.get_vocabulary())\n","index_to_word = tf.keras.layers.StringLookup(\n","    mask_token=\"\",\n","    vocabulary=tokenizer.get_vocabulary(),\n","    invert=True)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":507,"status":"ok","timestamp":1683097502486,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"qo-cfCX3LnHs","outputId":"a5062172-5e9f-4679-a697-64921e44c60a"},"outputs":[{"data":{"text/plain":["[[b'[START]', b'a', b'cat', b'in', b'a', b'hat', b'[END]'],\n"," [b'[START]', b'a', b'robot', b'dog', b'[END]']]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["w = index_to_word(t)\n","w.to_list()"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1683097508139,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"rrUUfGc65vAT","outputId":"9e468c76-57cf-4af8-98e8-316df613262a"},"outputs":[{"data":{"text/plain":["array([b'[START] a cat in a hat [END]', b'[START] a robot dog [END]'],\n","      dtype=object)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uEWM9xrYcg45"},"source":["### Prepare the datasets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6aX0Z_98S2tN"},"source":["The `train_raw` and `test_raw` datasets contain 1:many `(image, captions)` pairs. \n","\n","This function will replicate the image so there are 1:1 images to captions:"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683097508139,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"3_Lqwl9NiGT0"},"outputs":[],"source":["# def match_shapes(images, captions):\n","#   caption_shape = einops.parse_shape(captions, 'b c')\n","#   captions = einops.rearrange(captions, 'b c -> (b c)')\n","#   images = einops.repeat(\n","#       images, 'b ... -> (b c) ...',\n","#       c = caption_shape['c'])\n","#   return images, captions"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1989,"status":"ok","timestamp":1683097512355,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"CZGUsuGzUfzt","outputId":"d459f9bd-ad0e-41a2-8787-cdecd8298f55"},"outputs":[{"name":"stdout","output_type":"stream","text":["image paths: (32,)\n","captions: (32, 5)\n","\n","image_paths: (160,)\n","captions: (160,)\n"]}],"source":["# for ex_paths, ex_captions in train_raw.batch(32).take(1):\n","#   break\n","\n","# print('image paths:', ex_paths.shape)\n","# print('captions:', ex_captions.shape)\n","# print()\n","\n","# ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n","\n","# print('image_paths:', ex_paths.shape)\n","# print('captions:', ex_captions.shape)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8ENR_-swVhnm"},"source":["To be compatible with keras training the dataset should contain `(inputs, labels)` pairs. For text generation the tokens are both an input and the labels, shifted by one step. This function will convert an `(images, texts)` pair to an `((images, input_tokens), label_tokens)` pair:"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1683097515521,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"2DsgQ_hZT4C2"},"outputs":[],"source":["def prepare_txt(imgs, txts):\n","  tokens = tokenizer(txts)\n","\n","  input_tokens = tokens[..., :-1]\n","  label_tokens = tokens[..., 1:]\n","  return (imgs, input_tokens), label_tokens"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DA1x2j0JXX-N"},"source":["This function adds operations to a dataset. The steps are:\n","\n","1. Load the images (and ignore images that fail to load).\n","2. Replicate images to match the number of captions.\n","3. Shuffle and rebatch the `image, caption` pairs.\n","4. Tokenize the text, shift the tokens and add `label_tokens`.\n","5. Convert the text from a `RaggedTensor` representation to padded dense `Tensor` representation."]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1683097517926,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"4_Pt9zldjQ0q"},"outputs":[],"source":["def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n","  # Load the images and make batches.\n","  ds = (ds\n","        .shuffle(10000)\n","        .map(lambda path, caption: (load_image(path), caption))\n","        .apply(tf.data.experimental.ignore_errors())\n","        .batch(batch_size))\n","\n","  def to_tensor(inputs, labels):\n","    (images, in_tok), out_tok = inputs, labels\n","    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n","\n","  return (ds\n","          .map(match_shapes, tf.data.AUTOTUNE)\n","          .unbatch()\n","          .shuffle(shuffle_buffer)\n","          .batch(batch_size)\n","          .map(prepare_txt, tf.data.AUTOTUNE)\n","          .map(to_tensor, tf.data.AUTOTUNE)\n","          )"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LrQ85t1GNfpQ"},"source":["You could install the feature extractor in your model and train on the datasets like this:"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1356,"status":"ok","timestamp":1683097521111,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"1KlhOG5cjQ0r","outputId":"f31ac59c-530d-4c14-f5c6-bdfcc852f530"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-27-03f5d7fa769a>:6: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.ignore_errors` instead.\n"]},{"data":{"text/plain":["((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n","  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n"," TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["train_ds = prepare_dataset(train_raw, tokenizer)\n","train_ds.element_spec"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1075,"status":"ok","timestamp":1683097523247,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"d7Zy9F3zX7i2","outputId":"5142ecc2-b547-46c0-a675-9d0802fa35d6"},"outputs":[{"data":{"text/plain":["((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n","  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n"," TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["test_ds = prepare_dataset(test_raw, tokenizer)\n","test_ds.element_spec"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XZyKygJ8S8zW"},"source":["### [Optional] Cache the image features"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eHKhSKhti6NS"},"source":["Since the image feature extractor is not changing, and this tutorial is not using image augmentation, the image features can be cached. Same for the text tokenization. The time it takes to set up the cache is earned back on each epoch during training and validation. The code below defines two functions `save_dataset` and `load_dataset`: "]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":389,"status":"ok","timestamp":1683097525766,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"9N1MX5ym6xm5"},"outputs":[],"source":["def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n","  # Load the images and make batches.\n","  ds = (ds\n","        .map(lambda path, caption: (load_image(path), caption))\n","        .apply(tf.data.experimental.ignore_errors())\n","        .batch(batch_size))\n","\n","  # Run the feature extractor on each batch\n","  # Don't do this in a .map, because tf.data runs on the CPU. \n","  def gen():\n","    for (images, captions) in tqdm.tqdm(ds): \n","      feature_maps = image_model(images)\n","\n","      feature_maps, captions = match_shapes(feature_maps, captions)\n","      yield feature_maps, captions\n","\n","  # Wrap the generator in a new tf.data.Dataset.\n","  new_ds = tf.data.Dataset.from_generator(\n","      gen,\n","      output_signature=(\n","          tf.TensorSpec(shape=image_model.output_shape),\n","          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n","\n","  # Apply the tokenization \n","  new_ds = (new_ds\n","            .map(prepare_txt, tf.data.AUTOTUNE)\n","            .unbatch()\n","            .shuffle(1000))\n","\n","  # Save the dataset into shard files.\n","  def shard_func(i, item):\n","    return i % shards\n","  new_ds.enumerate().save(save_path, shard_func=shard_func)\n","\n","def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n","  def custom_reader_func(datasets):\n","    datasets = datasets.shuffle(1000)\n","    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n","  \n","  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n","\n","  def drop_index(i, x):\n","    return x\n","\n","  ds = (ds\n","        .map(drop_index, tf.data.AUTOTUNE)\n","        .shuffle(shuffle)\n","        .padded_batch(batch_size)\n","        .prefetch(tf.data.AUTOTUNE))\n","  return ds"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"tNdzrenxB3Yy"},"outputs":[{"name":"stderr","output_type":"stream","text":["79it [00:19,  4.15it/s]"]}],"source":["save_dataset(train_raw, 'train_cache', mobilenet, tokenizer)\n","save_dataset(test_raw, 'test_cache', mobilenet, tokenizer)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"798DtfH51UI8"},"source":[" </section>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GI265LiDslr2"},"source":["## Data ready for training\n","\n","After those preprocessing steps, here are the datasets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pwic2YCjHZmV"},"outputs":[],"source":["train_ds = load_dataset('train_cache')\n","test_ds = load_dataset('test_cache')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1682968092085,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"3B80JXj7HloX","outputId":"864f3a3f-4728-4fb0-d131-a53df4367b6c"},"outputs":[{"data":{"text/plain":["((TensorSpec(shape=(None, 7, 7, 576), dtype=tf.float32, name=None),\n","  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n"," TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["train_ds.element_spec"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5jfb8qknlsKi"},"source":["The dataset now returns `(input, label)` pairs suitable for training with keras. The `inputs` are `(images, input_tokens)` pairs. The `images` have been processed with the feature-extractor model. For each location in the `input_tokens` the model looks at the text so far and tries to predict the next which is lined up at the same location in the `labels`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1682968092085,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"YJBEwuXLZQdw","outputId":"b027daf6-6c85-4c29-9e56-9afd23363305"},"outputs":[{"name":"stdout","output_type":"stream","text":["(32, 7, 7, 576)\n","(32, 17)\n","(32, 17)\n"]}],"source":["for (inputs, ex_labels) in train_ds.take(1):\n","  (ex_img, ex_in_tok) = inputs\n","\n","print(ex_img.shape)\n","print(ex_in_tok.shape)\n","print(ex_labels.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"22R58DzZoF17"},"source":["The input tokens and the labels are the same, just shifted by 1 step:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1682968092086,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"V7h5UGftn1hT","outputId":"f0d83c03-08e7-4826-a78e-734226d95c98"},"outputs":[{"name":"stdout","output_type":"stream","text":["[  3   6  19   8 109   5 353  15 328   2 141 672   0   0   0   0   0]\n","[  6  19   8 109   5 353  15 328   2 141 672   4   0   0   0   0   0]\n"]}],"source":["print(ex_in_tok[0].numpy())\n","print(ex_labels[0].numpy())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DfICM49WFpIb"},"source":["## A Transformer decoder model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ONyjuWsmZoyO"},"source":["This model assumes that the pretrained image encoder is sufficient, and just focuses on building the text decoder. This tutorial uses a 2-layer Transformer-decoder.\n","\n","The implementations are almost identical to those in the [Transformers tutorial](https://www.tensorflow.org/text/tutorials/transformer). Refer back to it for more details.\n","\n","<table>\n","<tr>\n","  <th>The Transformer encoder and decoder.</th>\n","</tr>\n","<tr>\n","  <td>\n","   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n","  </td>\n","</tr>\n","</table>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qiRXWwIKNybB"},"source":["The model will be implemented in three main parts: \n","\n","1. Input - The token embedding and positional encoding (`SeqEmbedding`).\n","1. Decoder - A stack of transformer decoder layers (`DecoderLayer`) where each contains:\n","   1. A causal self attention later (`CausalSelfAttention`), where each output location can attend to the output so far.\n","   1. A cross attention layer (`CrossAttention`) where each output location can attend to the input image.\n","   1. A feed forward network (`FeedForward`) layer which further processes each output location independently.\n","1. Output - A multiclass-classification over the output vocabulary.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_ngm3SQMCaYU"},"source":["### Input"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"i9suaARZGPKw"},"source":["The input text has already been split up into tokens and converted to sequences of IDs. \n","\n","Remember that unlike a CNN or RNN the Transformer's attention layers are invariant to the order of the sequence. Without some positional input, it just sees an unordered set not a sequence. So in addition to a simple vector embedding for each token ID, the embedding layer will also include an embedding for each position in the sequence.\n","\n","The `SeqEmbedding` layer defined below:\n","\n","- It looks up the embedding vector for each token.\n","- It looks up an embedding vector for each sequence location.\n","- It adds the two together.\n","- It uses `mask_zero=True` to initialize the keras-masks for the model.\n","\n","Note: This implementation learns the position embeddings instead of using fixed embeddings like in the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer). Learning the embeddings is slightly less code, but doesn't generalize to longer sequences."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P91LU2F0a9Ga"},"outputs":[],"source":["class SeqEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, vocab_size, max_length, depth):\n","    super().__init__()\n","    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n","\n","    self.token_embedding = tf.keras.layers.Embedding(\n","        input_dim=vocab_size,\n","        output_dim=depth,\n","        mask_zero=True)\n","    \n","    self.add = tf.keras.layers.Add()\n","\n","  def call(self, seq):\n","    seq = self.token_embedding(seq) # (batch, seq, depth)\n","\n","    x = tf.range(tf.shape(seq)[1])  # (seq)\n","    x = x[tf.newaxis, :]  # (1, seq)\n","    x = self.pos_embedding(x)  # (1, seq, depth)\n","\n","    return self.add([seq,x])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"II1mD-bBCdMB"},"source":["### Decoder"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GHMLeMtKPTCW"},"source":["The decoder is a standard Transformer-decoder, it contains a stack of `DecoderLayers` where each contains three sublayers: a `CausalSelfAttention`, a `CrossAttention`, and a`FeedForward`. The implementations are almost identical to the [Transformer tutorial](https://www.tensorflow.org/text/tutorials/transformer), refer to it for more details.\n","\n","The `CausalSelfAttention` layer is below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JTLiX3lKooQ"},"outputs":[],"source":["class CausalSelfAttention(tf.keras.layers.Layer):\n","  def __init__(self, **kwargs):\n","    super().__init__()\n","    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n","    # Use Add instead of + so the keras mask propagates through.\n","    self.add = tf.keras.layers.Add() \n","    self.layernorm = tf.keras.layers.LayerNormalization()\n","  \n","  def call(self, x):\n","    attn = self.mha(query=x, value=x,\n","                    use_causal_mask=True)\n","    x = self.add([x, attn])\n","    return self.layernorm(x)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8c66OTRwQfd8"},"source":["The `CrossAttention` layer is below. Note the use of `return_attention_scores`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rIY6Vu2pLBAO"},"outputs":[],"source":["class CrossAttention(tf.keras.layers.Layer):\n","  def __init__(self,**kwargs):\n","    super().__init__()\n","    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n","    self.add = tf.keras.layers.Add() \n","    self.layernorm = tf.keras.layers.LayerNormalization()\n","  \n","  def call(self, x, y, **kwargs):\n","    attn, attention_scores = self.mha(\n","             query=x, value=y,\n","             return_attention_scores=True)\n","    \n","    self.last_attention_scores = attention_scores\n","\n","    x = self.add([x, attn])\n","    return self.layernorm(x)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8Hn5p6f-RE0C"},"source":["The `FeedForward` layer is below. Remember that a `layers.Dense` layer is applied to the last axis of the input. The input will have a shape of `(batch, sequence, channels)`, so it automatically applies pointwise across the `batch` and `sequence` axes.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWKrl7teOnH2"},"outputs":[],"source":["class FeedForward(tf.keras.layers.Layer):\n","  def __init__(self, units, dropout_rate=0.1):\n","    super().__init__()\n","    self.seq = tf.keras.Sequential([\n","        tf.keras.layers.Dense(units=2*units, activation='relu'),\n","        tf.keras.layers.Dense(units=units),\n","        tf.keras.layers.Dropout(rate=dropout_rate),\n","    ])\n","\n","    self.layernorm = tf.keras.layers.LayerNormalization()\n","  \n","  def call(self, x):\n","    x = x + self.seq(x)\n","    return self.layernorm(x)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lbXoiVNPRoJc"},"source":["Next arrange these three layers into a larger `DecoderLayer`. Each decoder layer applies the three smaller layers in sequence. After each sublayer the shape of `out_seq` is `(batch, sequence, channels)`. The decoder layer also returns the `attention_scores` for later visualizations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydcW5KZZHou7"},"outputs":[],"source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n","    super().__init__()\n","    \n","    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n","                                              key_dim=units,\n","                                              dropout=dropout_rate)\n","    self.cross_attention = CrossAttention(num_heads=num_heads,\n","                                          key_dim=units,\n","                                          dropout=dropout_rate)\n","    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n","      \n","\n","  def call(self, inputs, training=False):\n","    in_seq, out_seq = inputs\n","\n","    # Text input\n","    out_seq = self.self_attention(out_seq)\n","\n","    out_seq = self.cross_attention(out_seq, in_seq)\n","    \n","    self.last_attention_scores = self.cross_attention.last_attention_scores\n","\n","    out_seq = self.ff(out_seq)\n","\n","    return out_seq"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-lgbYrF5Csqu"},"source":["### Output"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VcnKZkrklAQf"},"source":["At minimum the output layer needs a `layers.Dense` layer to generate logit-predictions for each token at each location."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6WQD87efena5"},"source":["But there are a few other features you can add to make this work a little better:\n","\n","1. **Handle bad tokens**: The model will be generating text. It should\n","   never generate a pad, unknown, or start token (`''`, `'[UNK]'`, \n","   `'[START]'`). So set the bias for these to a large negative value.\n","\n","   > Note: You'll need to ignore these tokens in the loss function as well. \n","\n","2. **Smart initialization**: The default initialization of a dense layer will\n","  give a model that initially predicts each token with almost uniform\n","  likelihood. The actual token distribution is far from uniform. The\n","  optimal value for the initial bias of the output layer is the log of the\n","  probability of each token. So include an `adapt` method to count the tokens\n","  and set the optimal initial bias. This reduces the initial loss from the\n","  entropy of the uniform distribution (`log(vocabulary_size)`) to the marginal\n","  entropy of the distribution (`-p*log(p)`).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CeWw2SFDHUfo"},"outputs":[],"source":["#@title\n","class TokenOutput(tf.keras.layers.Layer):\n","  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n","    super().__init__()\n","    \n","    self.dense = tf.keras.layers.Dense(\n","        units=tokenizer.vocabulary_size(), **kwargs)\n","    self.tokenizer = tokenizer\n","    self.banned_tokens = banned_tokens\n","\n","    self.bias = None\n","\n","  def adapt(self, ds):\n","    counts = collections.Counter()\n","    vocab_dict = {name: id \n","                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n","\n","    for tokens in tqdm.tqdm(ds):\n","      counts.update(tokens.numpy().flatten())\n","\n","    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n","    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n","\n","    counts_arr = counts_arr[:]\n","    for token in self.banned_tokens:\n","      counts_arr[vocab_dict[token]] = 0\n","\n","    total = counts_arr.sum()\n","    p = counts_arr/total\n","    p[counts_arr==0] = 1.0\n","    log_p = np.log(p)  # log(1) == 0\n","\n","    entropy = -(log_p*p).sum()\n","\n","    print()\n","    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n","    print(f\"Marginal entropy: {entropy:0.2f}\")\n","\n","    self.bias = log_p\n","    self.bias[counts_arr==0] = -1e9\n","\n","  def call(self, x):\n","    x = self.dense(x)\n","    # TODO(b/250038731): Fix this.\n","    # An Add layer doesn't work because of the different shapes.\n","    # This clears the mask, that's okay because it prevents keras from rescaling\n","    # the losses.\n","    return x + self.bias\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xzQHqANd1A6Q"},"source":["The smart initialization will significantly reduce the initial loss:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20562,"status":"ok","timestamp":1682968198202,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"GGnOQyc501B2","outputId":"97a4c5df-bf05-4e3c-df51-d9617171e7f6"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 938/938 [00:20<00:00, 45.79it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Uniform entropy: 8.52\n","Marginal entropy: 5.29\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n","# This might run a little faster if the dataset didn't also have to load the image data.\n","output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3gq-ICN7bD-u"},"source":["### Build the model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gou4fPH_SWgH"},"source":["To build the model, you need to combine several parts:\n","\n","1. The image `feature_extractor` and the text `tokenizer` and.\n","1. The `seq_embedding` layer, to convert batches of token-IDs to \n","   vectors `(batch, sequence, channels)`.\n","3. The stack of `DecoderLayers` layers that will process the text and image data.\n","4. The `output_layer` which returns a pointwise prediction of what the next word should be."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHCISYehH1f6"},"outputs":[],"source":["class Captioner(tf.keras.Model):\n","  @classmethod\n","  def add_method(cls, fun):\n","    setattr(cls, fun.__name__, fun)\n","    return fun\n","\n","  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n","               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n","    super().__init__()\n","    self.feature_extractor = feature_extractor\n","    self.tokenizer = tokenizer\n","    self.word_to_index = tf.keras.layers.StringLookup(\n","        mask_token=\"\",\n","        vocabulary=tokenizer.get_vocabulary())\n","    self.index_to_word = tf.keras.layers.StringLookup(\n","        mask_token=\"\",\n","        vocabulary=tokenizer.get_vocabulary(),\n","        invert=True) \n","\n","    self.seq_embedding = SeqEmbedding(\n","        vocab_size=tokenizer.vocabulary_size(),\n","        depth=units,\n","        max_length=max_length)\n","\n","    self.decoder_layers = [\n","        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n","        for n in range(num_layers)]\n","\n","    self.output_layer = output_layer"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YW390dOz9T-x"},"source":["When you call the model, for training, it receives an `image, txt` pair. To make this function more usable, be flexible about the input:\n","\n","* If the image has 3 channels run it through the feature_extractor. Otherwise assume that it has been already. Similarly\n","* If the text has dtype `tf.string` run it through the tokenizer.\n","\n","After that running the model is only a few steps:\n","\n","1. Flatten the extracted image features, so they can be input to the decoder layers.\n","2. Look up the token embeddings.\n","3. Run the stack of `DecoderLayer`s, on the image features and text embeddings.\n","4. Run the output layer to predict the next token at each position.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lPdb7I4h9Ulo"},"outputs":[],"source":["  @Captioner.add_method\n","  def call(self, inputs):\n","    image, txt = inputs\n","\n","    if image.shape[-1] == 3:\n","      # Apply the feature-extractor, if you get an RGB image.\n","      image = self.feature_extractor(image)\n","    \n","    # Flatten the feature map\n","    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n","\n","\n","    if txt.dtype == tf.string:\n","      # Apply the tokenizer if you get string inputs.\n","      txt = tokenizer(txt)\n","\n","    txt = self.seq_embedding(txt)\n","\n","    # Look at the image\n","    for dec_layer in self.decoder_layers:\n","      txt = dec_layer(inputs=(image, txt))\n","      \n","    txt = self.output_layer(txt)\n","\n","    return txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmM7aZQsLiyU"},"outputs":[],"source":["model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n","                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xGvOcLQKghXN"},"source":["### Generate captions\n","\n","Before getting into training, write a bit of code to generate captions. You'll use this to see how training is progressing.\n","\n","Start by downloading a test image:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":705,"status":"ok","timestamp":1682968198902,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"cwFcdMqC-jE2","outputId":"e9857b31-f0d6-4dc7-a9bf-f6dc1516ee99"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://tensorflow.org/images/surf.jpg\n","64400/64400 [==============================] - 0s 3us/step\n"]}],"source":["image_url = 'https://tensorflow.org/images/surf.jpg'\n","image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n","image = load_image(image_path)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IRBIiTkubmxA"},"source":["To caption an image with this model:\n","\n","- Extract the `img_features`\n","- Initialize the list of output tokens with a `[START]` token.\n","- Pass `img_features` and `tokens` into the model.\n","  - It returns a list of logits.\n","  - Choose the next token based on those logits.  \n","  - Add it to the list of tokens, and continue the loop.\n","  - If it generates an `'[END]'` token, break out of the loop.\n","\n","So add a \"simple\" method to do just that:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nf1Jie9ef_Cg"},"outputs":[],"source":["@Captioner.add_method\n","def simple_gen(self, image, temperature=1):\n","  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n","  img_features = self.feature_extractor(image[tf.newaxis, ...])\n","\n","  tokens = initial # (batch, sequence)\n","  for n in range(50):\n","    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n","    preds = preds[:,-1, :]  #(batch, vocab)\n","    if temperature==0:\n","        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n","    else:\n","        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n","    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n","\n","    if next[0] == self.word_to_index('[END]'):\n","      break\n","  words = index_to_word(tokens[0, 1:-1])\n","  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n","  return result.numpy().decode()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TxN2NPX2zB8y"},"source":["Here are some generated captions for that image, the model's untrained, so they don't make much sense yet:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6461,"status":"ok","timestamp":1682968217119,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"sPm96CccvHnq","outputId":"01fd66b4-7460-4f7b-a2cb-d8b732521130"},"outputs":[{"name":"stdout","output_type":"stream","text":["a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n","the a a two a a\n","another a his dog the the rock a bubbles woman the grey clothes silly the a on holds sunset sitting pool a dog\n"]}],"source":["for t in (0.0, 0.5, 1.0):\n","  result = model.simple_gen(image, temperature=t)\n","  print(result)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JefwCRZ8z-Ah"},"source":["The temperature parameter allows you to interpolate between 3 modes:\n","\n","1. Greedy decoding (`temperature=0.0`) - Chooses the most likely next token at each step.\n","2. Random sampling according to the logits (`temperature=1.0`).\n","3. Uniform random sampling (`temperature >> 1.0`). \n","\n","Since the model is untrained, and it used the frequency-based initialization, the \"greedy\" output (first) usually only contains the most common tokens: `['a', '.', '[END]']`."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"r0FpTvaPkqON"},"source":["## Train"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IKcwZdqObK-U"},"source":["To train the model you'll need several additional components:\n","\n","- The Loss and metrics\n","- The Optimizer\n","- Optional Callbacks"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"g5IW2mWa2sAG"},"source":["### Losses and metrics"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XbpbDQTw1lOW"},"source":["Here's an implementation of a masked loss and accuracy:\n","\n","When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s24im3FqxAfT"},"outputs":[],"source":["def masked_loss(labels, preds):  \n","  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n","\n","  mask = (labels != 0) & (loss < 1e8) \n","  mask = tf.cast(mask, loss.dtype)\n","\n","  loss = loss*mask\n","  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n","  return loss\n","\n","def masked_acc(labels, preds):\n","  mask = tf.cast(labels!=0, tf.float32)\n","  preds = tf.argmax(preds, axis=-1)\n","  labels = tf.cast(labels, tf.int64)\n","  match = tf.cast(preds == labels, mask.dtype)\n","  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n","  return acc"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zOhjHqgv3F2e"},"source":["### Callbacks"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3dyQN9UfJYEd"},"source":["For feedback during training setup a `keras.callbacks.Callback` to generate some captions for the surfer image at the end of each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKDwbZOCZ-AP"},"outputs":[],"source":["class GenerateText(tf.keras.callbacks.Callback):\n","  def __init__(self):\n","    image_url = 'https://tensorflow.org/images/surf.jpg'\n","    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n","    self.image = load_image(image_path)\n","\n","  def on_epoch_end(self, epochs=None, logs=None):\n","    print()\n","    print()\n","    for t in (0.0, 0.5, 1.0):\n","      result = self.model.simple_gen(self.image, temperature=t)\n","      print(result)\n","    print()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1yNA3_RAsdl0"},"source":["It generates three output strings, like the earlier example, like before the first is \"greedy\", choosing the argmax of the logits at each step."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3737,"status":"ok","timestamp":1682968227258,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"IGVLpzo13rcA","outputId":"82a5ae24-64a2-4c3e-b6bf-bcf7e0cff408"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n","in a\n","three a bike many field sled who on sits wearing lake after in fighting wave two a\n","\n"]}],"source":["g = GenerateText()\n","g.model = model\n","g.on_epoch_end(0)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MAxp4KZRKDk9"},"source":["Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MjzrwGZp23xx"},"outputs":[],"source":["callbacks = [\n","    GenerateText(),\n","    tf.keras.callbacks.EarlyStopping(\n","        patience=5, restore_best_weights=True)]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZBaJhQpcG8u0"},"source":["### Train"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WBXG0dCDKO55"},"source":["Configure and execute the training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2OR5ZpAII__u"},"outputs":[],"source":["model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n","           loss=masked_loss,\n","           metrics=[masked_acc])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ro955bQ2KR0X"},"source":["For more frequent reporting, use the `Dataset.repeat()` method, and set the `steps_per_epoch` and `validation_steps` arguments to `Model.fit`. \n","\n","With this setup on `Flickr8k` a full pass over the dataset is 900+ batches, but below the reporting-epochs are 100 steps."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":489835,"status":"ok","timestamp":1682968733062,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"3aB0baOVMZe9","outputId":"440651e2-3527-47b3-f4e4-ff310efb1a52"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 5.0186 - masked_acc: 0.1943\n","\n","a man in a man in the water\n","a man in a man in a dog in a\n","surfer through grabs man fountains the and an red player deck distance\n","\n","100/100 [==============================] - 43s 271ms/step - loss: 5.0165 - masked_acc: 0.1946 - val_loss: 4.6595 - val_masked_acc: 0.2438\n","Epoch 2/100\n","100/100 [==============================] - ETA: 0s - loss: 4.6227 - masked_acc: 0.2528\n","\n","a man in a white dog is in the water\n","a man with a little with a woman in a woman\n","skateboarder of the water\n","\n","100/100 [==============================] - 14s 146ms/step - loss: 4.6227 - masked_acc: 0.2528 - val_loss: 4.3419 - val_masked_acc: 0.2806\n","Epoch 3/100\n","100/100 [==============================] - ETA: 0s - loss: 4.4059 - masked_acc: 0.2780\n","\n","a man in a white and a man in a white dog is\n","a man is is running in a mouth\n","a group outfit in a dog on a snow\n","\n","100/100 [==============================] - 14s 140ms/step - loss: 4.4059 - masked_acc: 0.2780 - val_loss: 4.1894 - val_masked_acc: 0.2920\n","Epoch 4/100\n","100/100 [==============================] - ETA: 0s - loss: 4.2690 - masked_acc: 0.2915\n","\n","a man in a red shirt is running in the water\n","a man in the beach\n","the race brown dog holding a group of a pants of water\n","\n","100/100 [==============================] - 11s 107ms/step - loss: 4.2690 - masked_acc: 0.2915 - val_loss: 4.0141 - val_masked_acc: 0.3052\n","Epoch 5/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 4.1426 - masked_acc: 0.3018\n","\n","a man in a white shirt is running in the water\n","a man in a person in the water\n","a man in a playground the water on a long brown bottle\n","\n","100/100 [==============================] - 8s 78ms/step - loss: 4.1423 - masked_acc: 0.3023 - val_loss: 3.8982 - val_masked_acc: 0.3251\n","Epoch 6/100\n","100/100 [==============================] - ETA: 0s - loss: 4.0089 - masked_acc: 0.3176\n","\n","a man in a red shirt is running in the water\n","a man in a hat is running in the water\n","four performing a boy down a dog is jumping through the water in the beach\n","\n","100/100 [==============================] - 11s 107ms/step - loss: 4.0089 - masked_acc: 0.3176 - val_loss: 3.8709 - val_masked_acc: 0.3173\n","Epoch 7/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.9398 - masked_acc: 0.3224\n","\n","a man in a red shirt is running in the water\n","a man in a a snowy is riding a red coat in the water\n","a man is tupperware shot\n","\n","100/100 [==============================] - 7s 75ms/step - loss: 3.9402 - masked_acc: 0.3223 - val_loss: 3.7974 - val_masked_acc: 0.3268\n","Epoch 8/100\n","100/100 [==============================] - ETA: 0s - loss: 3.8930 - masked_acc: 0.3252\n","\n","a man in a blue shirt is running through the water\n","a man in a black shirt is running on a blue pool\n","two people in water facing across the tie\n","\n","100/100 [==============================] - 14s 144ms/step - loss: 3.8930 - masked_acc: 0.3252 - val_loss: 3.7516 - val_masked_acc: 0.3299\n","Epoch 9/100\n","100/100 [==============================] - ETA: 0s - loss: 3.8054 - masked_acc: 0.3345\n","\n","a man in a blue shirt is jumping in the water\n","a child in the water\n","the girl wearing a same talking onto yellow pool\n","\n","100/100 [==============================] - 16s 164ms/step - loss: 3.8054 - masked_acc: 0.3345 - val_loss: 3.6398 - val_masked_acc: 0.3440\n","Epoch 10/100\n","100/100 [==============================] - ETA: 0s - loss: 3.7280 - masked_acc: 0.3418\n","\n","a man in a red shirt is jumping in the water\n","a man is standing on a beach\n","man wearing leaping down the water\n","\n","100/100 [==============================] - 7s 66ms/step - loss: 3.7280 - masked_acc: 0.3418 - val_loss: 3.6081 - val_masked_acc: 0.3372\n","Epoch 11/100\n","100/100 [==============================] - ETA: 0s - loss: 3.6479 - masked_acc: 0.3478\n","\n","a man in a blue shirt is jumping in a pool\n","a man in a red water\n","a player rides a in the water stage\n","\n","100/100 [==============================] - 10s 96ms/step - loss: 3.6479 - masked_acc: 0.3478 - val_loss: 3.5485 - val_masked_acc: 0.3487\n","Epoch 12/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.6107 - masked_acc: 0.3503\n","\n","a man in a red shirt is jumping into the water\n","man in red pool\n","a kids are do a pool\n","\n","100/100 [==============================] - 6s 62ms/step - loss: 3.6088 - masked_acc: 0.3507 - val_loss: 3.4913 - val_masked_acc: 0.3507\n","Epoch 13/100\n","100/100 [==============================] - ETA: 0s - loss: 3.5960 - masked_acc: 0.3471\n","\n","a man in a red shirt is jumping into the water\n","a man wearing a red shirt is jumping into the ocean\n","a boy sharp in a pool at a wave\n","\n","100/100 [==============================] - 10s 96ms/step - loss: 3.5960 - masked_acc: 0.3471 - val_loss: 3.5133 - val_masked_acc: 0.3405\n","Epoch 14/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.5697 - masked_acc: 0.3499\n","\n","a man in a blue shirt is jumping into the water\n","a man in a blue jacket is sitting on a wave\n","a man blue is in the edge of the lake\n","\n","100/100 [==============================] - 7s 69ms/step - loss: 3.5720 - masked_acc: 0.3499 - val_loss: 3.3972 - val_masked_acc: 0.3490\n","Epoch 15/100\n","100/100 [==============================] - ETA: 0s - loss: 3.4700 - masked_acc: 0.3604\n","\n","a man in a red shirt is swimming pool\n","a man in a blue and white suit stands in the water\n","a black in red on water rope wave\n","\n","100/100 [==============================] - 13s 131ms/step - loss: 3.4700 - masked_acc: 0.3604 - val_loss: 3.4533 - val_masked_acc: 0.3508\n","Epoch 16/100\n","100/100 [==============================] - ETA: 0s - loss: 3.4454 - masked_acc: 0.3611\n","\n","a man in a blue shirt is swimming pool\n","a man in a white shirt is carrying a swimming pool\n","a man wearing a yellow swimming dog jumping off\n","\n","100/100 [==============================] - 14s 140ms/step - loss: 3.4454 - masked_acc: 0.3611 - val_loss: 3.3915 - val_masked_acc: 0.3585\n","Epoch 17/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.4485 - masked_acc: 0.3599\n","\n","a man in a red shirt is swimming pool\n","a man in a red jacket is jumping into a pool\n","a man in a blue pants jumps into a wet swing\n","\n","100/100 [==============================] - 7s 73ms/step - loss: 3.4481 - masked_acc: 0.3599 - val_loss: 3.3298 - val_masked_acc: 0.3624\n","Epoch 18/100\n","100/100 [==============================] - ETA: 0s - loss: 3.3891 - masked_acc: 0.3658\n","\n","a man in a blue shirt is swimming pool\n","a man is playing in the water\n","two boys on a wave\n","\n","100/100 [==============================] - 9s 86ms/step - loss: 3.3891 - masked_acc: 0.3658 - val_loss: 3.2973 - val_masked_acc: 0.3656\n","Epoch 19/100\n","100/100 [==============================] - ETA: 0s - loss: 3.3675 - masked_acc: 0.3669\n","\n","a man in a red shirt is swimming pool\n","a man in a surfer a blue wave\n","a man dressed in a blue swimming wave\n","\n","100/100 [==============================] - 8s 77ms/step - loss: 3.3675 - masked_acc: 0.3669 - val_loss: 3.3403 - val_masked_acc: 0.3545\n","Epoch 20/100\n","100/100 [==============================] - ETA: 0s - loss: 3.3186 - masked_acc: 0.3693\n","\n","a man in a blue shirt is swimming in the water\n","a man in a blue shirt is playing in the water\n","the tan is ice out of surfboard\n","\n","100/100 [==============================] - 9s 86ms/step - loss: 3.3186 - masked_acc: 0.3693 - val_loss: 3.2067 - val_masked_acc: 0.3721\n","Epoch 21/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.2753 - masked_acc: 0.3755\n","\n","a man in a blue shirt is swimming pool\n","a man in a blue shirt is playing in the water\n","two adults in blue pool are swimming swimming pool\n","\n","100/100 [==============================] - 8s 75ms/step - loss: 3.2775 - masked_acc: 0.3752 - val_loss: 3.2109 - val_masked_acc: 0.3687\n","Epoch 22/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.2382 - masked_acc: 0.3773\n","\n","a man in a blue shirt is swimming pool\n","a man is wearing a blue shirt is swimming pool\n","a child in a red shirt has performs a stream\n","\n","100/100 [==============================] - 8s 78ms/step - loss: 3.2374 - masked_acc: 0.3775 - val_loss: 3.1976 - val_masked_acc: 0.3865\n","Epoch 23/100\n","100/100 [==============================] - ETA: 0s - loss: 3.2244 - masked_acc: 0.3797\n","\n","a man in a red shirt is swimming in the water\n","a person in a red is holding a wave\n","a person in a surfboard on a wave\n","\n","100/100 [==============================] - 8s 83ms/step - loss: 3.2244 - masked_acc: 0.3797 - val_loss: 3.1888 - val_masked_acc: 0.3749\n","Epoch 24/100\n","100/100 [==============================] - ETA: 0s - loss: 3.2206 - masked_acc: 0.3792\n","\n","a man in a blue shirt is swimming in the water\n","a person in a purple shirt is swimming in a wave\n","a man wearing a dangling in the air foot stands in the ocean\n","\n","100/100 [==============================] - 8s 76ms/step - loss: 3.2206 - masked_acc: 0.3792 - val_loss: 3.2585 - val_masked_acc: 0.3726\n","Epoch 25/100\n","100/100 [==============================] - ETA: 0s - loss: 3.2057 - masked_acc: 0.3787\n","\n","a man in a blue shirt is swimming pool\n","a man in a blue surfboard as he is swimming in a surfboard\n","a man in a uniform looking at the tube in the surf\n","\n","100/100 [==============================] - 9s 91ms/step - loss: 3.2057 - masked_acc: 0.3787 - val_loss: 3.1433 - val_masked_acc: 0.3772\n","Epoch 26/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.1780 - masked_acc: 0.3819\n","\n","a man in a red shirt is swimming pool\n","a man with a red and black dog is swimming in a blue water\n","a young girl in orange trunks and swimming smiles along the ocean\n","\n","100/100 [==============================] - 7s 74ms/step - loss: 3.1785 - masked_acc: 0.3817 - val_loss: 3.2154 - val_masked_acc: 0.3641\n","Epoch 27/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.1547 - masked_acc: 0.3829\n","\n","a man in a red shirt is swimming pool\n","a surfer in a red surfboard in the water\n","a man in a hat looks on the ocean with referee hits a hotel\n","\n","100/100 [==============================] - 10s 104ms/step - loss: 3.1539 - masked_acc: 0.3830 - val_loss: 3.0970 - val_masked_acc: 0.3811\n","Epoch 28/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.1784 - masked_acc: 0.3815\n","\n","a man in a yellow shirt is swimming in the water\n","a man in a yellow canoe is playing in a pool\n","a cliffs woman in a green wave in water\n","\n","100/100 [==============================] - 7s 75ms/step - loss: 3.1790 - masked_acc: 0.3817 - val_loss: 3.0947 - val_masked_acc: 0.3826\n","Epoch 29/100\n","100/100 [==============================] - ETA: 0s - loss: 3.0848 - masked_acc: 0.3885\n","\n","a man in a red shirt is swimming in a wave\n","a man in a red shirt is splashing in a wave\n","a surfer is sitting on wave\n","\n","100/100 [==============================] - 9s 92ms/step - loss: 3.0848 - masked_acc: 0.3885 - val_loss: 3.0664 - val_masked_acc: 0.3735\n","Epoch 30/100\n","100/100 [==============================] - ETA: 0s - loss: 3.0491 - masked_acc: 0.3926\n","\n","a man in a blue shirt is surfing\n","a person is surfing in a wave\n","a girl wearing a blue boogie swims into the ocean\n","\n","100/100 [==============================] - 7s 66ms/step - loss: 3.0491 - masked_acc: 0.3926 - val_loss: 3.0850 - val_masked_acc: 0.3808\n","Epoch 31/100\n","100/100 [==============================] - ETA: 0s - loss: 3.0593 - masked_acc: 0.3923\n","\n","a man in a red shirt is surfing\n","a man in a red kayak is surfing\n","three women in the swimming hat on a wave\n","\n","100/100 [==============================] - 9s 91ms/step - loss: 3.0593 - masked_acc: 0.3923 - val_loss: 3.1267 - val_masked_acc: 0.3792\n","Epoch 32/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.0515 - masked_acc: 0.3910\n","\n","a man in a red shirt is surfing\n","a man in a red and blue jacket is surfing\n","two people circular in white and blue outfits above through the surfboard\n","\n","100/100 [==============================] - 8s 81ms/step - loss: 3.0490 - masked_acc: 0.3912 - val_loss: 3.1359 - val_masked_acc: 0.3773\n","Epoch 33/100\n","100/100 [==============================] - ETA: 0s - loss: 3.0038 - masked_acc: 0.3972\n","\n","a man in a red shirt is surfing\n","a man in a blue shirt is surfing a wave\n","a biker enjoying a surfboard in a wave are surfing\n","\n","100/100 [==============================] - 10s 97ms/step - loss: 3.0038 - masked_acc: 0.3972 - val_loss: 3.0197 - val_masked_acc: 0.3799\n","Epoch 34/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 3.0084 - masked_acc: 0.3962\n","\n","a man in a blue wetsuit is swimming pool\n","a man in a wetsuit is splashing in the water\n","child with dark bathing suit is swim mouth has hood of tarp\n","\n","100/100 [==============================] - 7s 69ms/step - loss: 3.0088 - masked_acc: 0.3959 - val_loss: 3.0985 - val_masked_acc: 0.3726\n","Epoch 35/100\n","100/100 [==============================] - ETA: 0s - loss: 2.9912 - masked_acc: 0.3982\n","\n","a man in a red wetsuit is surfing\n","a surfer rides a wave\n","a group of people playing in the water\n","\n","100/100 [==============================] - 9s 92ms/step - loss: 2.9912 - masked_acc: 0.3982 - val_loss: 3.0549 - val_masked_acc: 0.3801\n","Epoch 36/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 2.9773 - masked_acc: 0.3987\n","\n","a man in a red wetsuit is surfing\n","a man wearing a red shirt is swimming in a pool\n","a surfer is surfing a huge wave\n","\n","100/100 [==============================] - 6s 65ms/step - loss: 2.9758 - masked_acc: 0.3988 - val_loss: 2.9856 - val_masked_acc: 0.3914\n","Epoch 37/100\n","100/100 [==============================] - ETA: 0s - loss: 2.9724 - masked_acc: 0.3966\n","\n","a man in a blue wetsuit is surfing\n","a person in a red suit is riding a wave\n","a surfer rides a surfboard\n","\n","100/100 [==============================] - 9s 90ms/step - loss: 2.9724 - masked_acc: 0.3966 - val_loss: 2.9891 - val_masked_acc: 0.3866\n","Epoch 38/100\n","100/100 [==============================] - ETA: 0s - loss: 2.9472 - masked_acc: 0.4027\n","\n","a man in a blue wetsuit is surfing\n","a man in a yellow surfboard is surfing in a wave\n","a woman wearing a surfboard that is suit and watches from a red rope in a white suit\n","\n","100/100 [==============================] - 7s 74ms/step - loss: 2.9472 - masked_acc: 0.4027 - val_loss: 2.9769 - val_masked_acc: 0.3853\n","Epoch 39/100\n","100/100 [==============================] - ETA: 0s - loss: 2.8754 - masked_acc: 0.4071\n","\n","a man in a yellow kayak is surfing\n","a surfer is surfing\n","a surfer is on a slide at a skateboard\n","\n","100/100 [==============================] - 10s 96ms/step - loss: 2.8754 - masked_acc: 0.4071 - val_loss: 3.0013 - val_masked_acc: 0.3842\n","Epoch 40/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 2.8913 - masked_acc: 0.4076\n","\n","a man in a red shirt is surfing a wave\n","a man in a blue wetsuit is in a pool\n","a man is riding a kayak on a running that is in a giant lower leather shallow wave\n","\n","100/100 [==============================] - 7s 74ms/step - loss: 2.8928 - masked_acc: 0.4074 - val_loss: 2.9271 - val_masked_acc: 0.3913\n","Epoch 41/100\n","100/100 [==============================] - ETA: 0s - loss: 2.8738 - masked_acc: 0.4096\n","\n","a man in a red shirt is surfing a wave\n","a young girl in a red surfboard in a yellow kayak\n","a surfer in the yellow wetsuit is jumping in the pool\n","\n","100/100 [==============================] - 9s 95ms/step - loss: 2.8738 - masked_acc: 0.4096 - val_loss: 2.9739 - val_masked_acc: 0.3876\n","Epoch 42/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 2.8592 - masked_acc: 0.4091\n","\n","a man in a blue wetsuit is surfing a wave\n","a man in a kayak on a surfboard\n","a skier riding a wave\n","\n","100/100 [==============================] - 6s 60ms/step - loss: 2.8582 - masked_acc: 0.4093 - val_loss: 2.9688 - val_masked_acc: 0.3979\n","Epoch 43/100\n","100/100 [==============================] - ETA: 0s - loss: 2.8388 - masked_acc: 0.4090\n","\n","a man in a red wetsuit is swimming pool\n","a man in a red wetsuit is swimming pool\n","surfers with running through the deep blue water\n","\n","100/100 [==============================] - 7s 75ms/step - loss: 2.8388 - masked_acc: 0.4090 - val_loss: 2.9499 - val_masked_acc: 0.3952\n","Epoch 44/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 2.8570 - masked_acc: 0.4134\n","\n","a surfer is surfing a wave\n","a surfer is going out of a wave\n","a surfer is riding a yellow and blue pool\n","\n","100/100 [==============================] - 7s 75ms/step - loss: 2.8584 - masked_acc: 0.4135 - val_loss: 2.8933 - val_masked_acc: 0.4008\n","Epoch 45/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 2.8398 - masked_acc: 0.4116\n","\n","a man in a red wetsuit is surfing\n","a man in a white surfboard is surfing\n","a guy rides a kayak on a surfboard\n","\n","100/100 [==============================] - 6s 62ms/step - loss: 2.8397 - masked_acc: 0.4113 - val_loss: 2.9487 - val_masked_acc: 0.3929\n","Epoch 46/100\n","100/100 [==============================] - ETA: 0s - loss: 2.8632 - masked_acc: 0.4043\n","\n","a man in a wetsuit is surfing\n","a surfer is surfing\n","a man in the kayak is surfing in a wave\n","\n","100/100 [==============================] - 9s 88ms/step - loss: 2.8632 - masked_acc: 0.4043 - val_loss: 2.8584 - val_masked_acc: 0.4005\n","Epoch 47/100\n","100/100 [==============================] - ETA: 0s - loss: 2.8384 - masked_acc: 0.4103\n","\n","a man in a yellow wetsuit is surfing\n","a man wearing a red wetsuit is surfing\n","a plane takes a while riding a wave\n","\n","100/100 [==============================] - 6s 65ms/step - loss: 2.8384 - masked_acc: 0.4103 - val_loss: 2.8414 - val_masked_acc: 0.4056\n","Epoch 48/100\n","100/100 [==============================] - ETA: 0s - loss: 2.7344 - masked_acc: 0.4224\n","\n","a man in a red wetsuit is surfing\n","a man in a red wetsuit is surfing on a wave\n","a toddler camp splash in the ocean\n","\n","100/100 [==============================] - 9s 88ms/step - loss: 2.7344 - masked_acc: 0.4224 - val_loss: 2.9328 - val_masked_acc: 0.3915\n","Epoch 49/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 2.7253 - masked_acc: 0.4216\n","\n","a man in a red wetsuit is surfing\n","a person in a wetsuit is surfing\n","a surfer in a red shirt rides at a wave\n","\n","100/100 [==============================] - 6s 63ms/step - loss: 2.7267 - masked_acc: 0.4215 - val_loss: 2.9561 - val_masked_acc: 0.3883\n","Epoch 50/100\n","100/100 [==============================] - ETA: 0s - loss: 2.7374 - masked_acc: 0.4191\n","\n","a man in a wetsuit is surfing\n","a surfer rides a wave\n","a man surfing in the ocean\n","\n","100/100 [==============================] - 8s 76ms/step - loss: 2.7374 - masked_acc: 0.4191 - val_loss: 2.9287 - val_masked_acc: 0.3977\n","Epoch 51/100\n","100/100 [==============================] - ETA: 0s - loss: 2.7528 - masked_acc: 0.4204\n","\n","a man in a blue wetsuit is surfing\n","a surfer rides a wave\n","a man in an orange kayak is being boat to balance out of a parachute\n","\n","100/100 [==============================] - 7s 73ms/step - loss: 2.7528 - masked_acc: 0.4204 - val_loss: 2.9006 - val_masked_acc: 0.3912\n","Epoch 52/100\n"," 99/100 [============================>.] - ETA: 0s - loss: 2.7296 - masked_acc: 0.4221\n","\n","a surfer rides a wave\n","a man in a red shirt is surfing\n","a person is waves on a yellow wave\n","\n","100/100 [==============================] - 7s 72ms/step - loss: 2.7289 - masked_acc: 0.4221 - val_loss: 2.8756 - val_masked_acc: 0.3976\n"]}],"source":["# history = model.fit(\n","#     train_ds.repeat(),\n","#     steps_per_epoch=100,\n","#     validation_data=test_ds.repeat(),\n","#     validation_steps=20,\n","#     epochs=100,\n","#     callbacks=callbacks)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"P634LfVgw-eV"},"source":["Plot the loss and accuracy over the training run:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"elapsed":32555,"status":"ok","timestamp":1682969174811,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"6Wn8KSkUw916","outputId":"505b1a9d-4963-42a1-8cc4-1d05bf1fa866"},"outputs":[{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7ff3ea6c6c80>"]},"execution_count":63,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.plot(history.history['loss'], label='loss')\n","plt.plot(history.history['val_loss'], label='val_loss')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('CE/token')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"elapsed":1002,"status":"ok","timestamp":1682969175806,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"yZQ78b2Kxw-T","outputId":"ab01e3e5-8031-4399-8161-3ada3b44e336"},"outputs":[{"data":{"text/plain":["<matplotlib.legend.Legend at 0x7ff3659be050>"]},"execution_count":64,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.plot(history.history['masked_acc'], label='accuracy')\n","plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n","plt.ylim([0, max(plt.ylim())])\n","plt.xlabel('Epoch #')\n","plt.ylabel('CE/token')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"elapsed":13009,"status":"error","timestamp":1682969195166,"user":{"displayName":"Joel Kunjachan Varghese","userId":"13276675938730244655"},"user_tz":-330},"id":"D9lNOEJ2uj1o","outputId":"c58554b1-fbe3-4eb0-d287-686b77dadf4b"},"outputs":[{"ename":"MessageError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kTQoC1MTu0Yl"},"outputs":[],"source":["model.save(\"/content/drive/image_caption/model.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ib9BKF_EyAou"},"outputs":[],"source":["model.save_weights('model_weights.weights')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SQN1qT7KNqbL"},"source":["## Attention plots"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"E9XJaC2b2J23"},"source":["Now, using the trained model,  run that `simple_gen` method on the image:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UQPtNTb2eu3"},"outputs":[],"source":["result = model.simple_gen(image, temperature=0.0)\n","result"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7NXbmeLGN1bJ"},"source":["Split the output back into tokens:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHKOpm0w5Xto"},"outputs":[],"source":["str_tokens = result.split()\n","str_tokens.append('[END]')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fE-AjuAV55Qo"},"source":["The `DecoderLayers` each cache the attention scores for their `CrossAttention` layer. The shape of each attention map is `(batch=1, heads, sequence, image)`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZpyuQvq2q-B"},"outputs":[],"source":["attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n","[map.shape for map in attn_maps]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T42ImsWv6oHG"},"source":["So stack the maps along the `batch` axis, then average over the `(batch, heads)` axes, while splitting the `image` axis back into `height, width`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojwtvnkh6mS-"},"outputs":[],"source":["attention_maps = tf.concat(attn_maps, axis=0)\n","attention_maps = einops.reduce(\n","    attention_maps,\n","    'batch heads sequence (height width) -> sequence height width',\n","    height=7, width=7,\n","    reduction='mean')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4TM7rA3zGpJW"},"source":["Now you have a single attention map, for each sequence prediction. The values in each map should sum to `1.`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASWmWerGCZp3"},"outputs":[],"source":["einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fv7XYGFUd-U7"},"source":["So here is where the model was focusing attention while generating each token of the output:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fD_y7PD6RPGt"},"outputs":[],"source":["def plot_attention_maps(image, str_tokens, attention_map):\n","    fig = plt.figure(figsize=(16, 9))\n","\n","    len_result = len(str_tokens)\n","    \n","    titles = []\n","    for i in range(len_result):\n","      map = attention_map[i]\n","      grid_size = max(int(np.ceil(len_result/2)), 2)\n","      ax = fig.add_subplot(3, grid_size, i+1)\n","      titles.append(ax.set_title(str_tokens[i]))\n","      img = ax.imshow(image)\n","      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n","                clim=[0.0, np.max(map)])\n","\n","    plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PI4NAAws9rvY"},"outputs":[],"source":["plot_attention_maps(image/255, str_tokens, attention_maps)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"riTz0abQKMkV"},"source":["Now put that together into a more usable function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mktpfW-SKQIJ"},"outputs":[],"source":["@Captioner.add_method\n","def run_and_show_attention(self, image, temperature=0.0):\n","  result_txt = self.simple_gen(image, temperature)\n","  str_tokens = result_txt.split()\n","  str_tokens.append('[END]')\n","\n","  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n","  attention_maps = tf.concat(attention_maps, axis=0)\n","  attention_maps = einops.reduce(\n","      attention_maps,\n","      'batch heads sequence (height width) -> sequence height width',\n","      height=7, width=7,\n","      reduction='mean')\n","  \n","  plot_attention_maps(image/255, str_tokens, attention_maps)\n","  t = plt.suptitle(result_txt)\n","  t.set_y(1.05)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FntRkY11OiMw"},"outputs":[],"source":["run_and_show_attention(model, image)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Rprk3HEvZuxb"},"source":["## Try it on your own images\n","\n","For fun, below you're provided a method you can use to caption your own images with the model you've just trained. Keep in mind, it was trained on a relatively small amount of data, and your images may be different from the training data (so be prepared for strange results!)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Psd1quzaAWg"},"outputs":[],"source":["image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n","image_path = tf.keras.utils.get_file(origin=image_url)\n","image = load_image(image_path)\n","\n","run_and_show_attention(model, image)"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":0}
